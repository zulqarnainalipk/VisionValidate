{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":87323,"databundleVersionId":9934427,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nimport torchvision.transforms as transforms\nimport timm\nfrom torch.utils.data import DataLoader, Dataset, random_split, Subset\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport pandas as pd\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T09:35:28.518186Z","iopub.execute_input":"2025-01-21T09:35:28.518434Z","iopub.status.idle":"2025-01-21T09:35:35.811490Z","shell.execute_reply.started":"2025-01-21T09:35:28.518406Z","shell.execute_reply":"2025-01-21T09:35:35.810576Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.17). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Dataset Class with Albumentations","metadata":{}},{"cell_type":"code","source":"class CustomImageDataset(Dataset):\n    def __init__(self, csv_file, img_dir, transform=None):\n        # Initialize the dataset with a CSV file and image directory\n        self.annotations = csv_file  # CSV file containing image paths and labels\n        self.img_dir = img_dir  # Directory where images are stored\n        self.transform = transform  # Optional transformations to apply to images\n\n    def __len__(self):\n        # Return the total number of samples in the dataset\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        # Get the image and its corresponding label for the given index (idx)\n        img_path = os.path.join(self.img_dir, self.annotations.iloc[idx, 0])  # Get image path\n        image = Image.open(img_path).convert(\"RGB\")  # Open image and convert it to RGB (3 channels)\n        \n        # Get the label (assuming label is in the second column of the CSV)\n        label = torch.tensor(int(self.annotations.iloc[idx, 1]))  # Convert label to tensor\n        \n        # Apply the transformation (if any)\n        if self.transform:\n            # Convert image to numpy array before applying transformation\n            image = np.array(image)\n            # Apply the transformation and retrieve the transformed image\n            image = self.transform(image=image)[\"image\"]\n        \n        # Return the transformed image and the corresponding label\n        return image, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T09:35:35.813138Z","iopub.execute_input":"2025-01-21T09:35:35.814209Z","iopub.status.idle":"2025-01-21T09:35:35.819605Z","shell.execute_reply.started":"2025-01-21T09:35:35.814178Z","shell.execute_reply":"2025-01-21T09:35:35.818849Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Define Data Augmentations","metadata":{}},{"cell_type":"code","source":"# Function to get data augmentation and normalization transforms\ndef get_transforms(img_size):\n    \"\"\"\n    Creates and returns training and validation image transformations.\n\n    Parameters:\n    img_size (tuple): Desired image size as (height, width).\n\n    Returns:\n    tuple: A tuple containing training and validation transformations.\n    \"\"\"\n    # Training transformations include data augmentation techniques to improve model generalization\n    train_transform = A.Compose([\n        A.Resize(img_size[0], img_size[1]),  # Resizes the image to the specified dimensions\n        A.HorizontalFlip(p=0.5),  # Randomly flips the image horizontally with a 50% probability\n        A.RandomBrightnessContrast(p=0.5),  # Adjusts brightness and contrast randomly with a 50% probability\n        A.HueSaturationValue(p=0.5),  # Randomly changes hue, saturation, and value with a 50% probability\n        A.GaussianBlur(p=0.3),  # Applies Gaussian blur to the image with a 30% probability\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalizes image pixel values\n        ToTensorV2(),  # Converts the image and its augmentations to a PyTorch tensor\n    ])\n    \n    # Validation transformations focus only on resizing and normalization for consistency\n    val_transform = A.Compose([\n        A.Resize(img_size[0], img_size[1]),  # Resizes the image to the specified dimensions\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalizes image pixel values\n        ToTensorV2(),  # Converts the image to a PyTorch tensor\n    ])\n    \n    return train_transform, val_transform\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T09:35:35.820485Z","iopub.execute_input":"2025-01-21T09:35:35.820721Z","iopub.status.idle":"2025-01-21T09:35:35.832664Z","shell.execute_reply.started":"2025-01-21T09:35:35.820697Z","shell.execute_reply":"2025-01-21T09:35:35.831863Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Create DataLoaders","metadata":{}},{"cell_type":"code","source":"def create_dataloaders(csv_file, img_dir, img_size=(224, 224), batch_size=32, n_fold=0):\n    \"\"\"\n    Creates DataLoader objects for training and validation datasets.\n\n    Parameters:\n    csv_file (pd.DataFrame): DataFrame containing image paths and labels.\n    img_dir (str): Directory where images are stored.\n    img_size (tuple): Desired image size as (height, width). Default is (224, 224).\n    batch_size (int): Number of samples per batch. Default is 32.\n    n_fold (int): Index of the fold for cross-validation. Default is 0.\n\n    Returns:\n    tuple: DataLoader objects for training and validation datasets.\n    \"\"\"\n    # Get training and validation transformations\n    train_transform, val_transform = get_transforms(img_size)\n    \n    # Initialize the dataset without any transformations\n    dataset = CustomImageDataset(csv_file=csv_file, img_dir=img_dir, transform=None)\n    \n    # Stratified K-Fold for splitting dataset into train and validation sets\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2024)\n    \n    # Iterate through folds to get train and validation indices for the specified fold\n    for i, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(csv_file)), csv_file.iloc[:, 1].values)):\n        if i == n_fold:\n            break\n    \n    # Subset datasets for training and validation with respective transformations applied\n    train_dataset = Subset(\n        CustomImageDataset(csv_file.iloc[train_idx], img_dir, transform=train_transform),\n        range(len(train_idx))\n    )\n    val_dataset = Subset(\n        CustomImageDataset(csv_file.iloc[val_idx], img_dir, transform=val_transform),\n        range(len(val_idx))\n    )\n    \n    # Create DataLoader for training dataset with shuffling and multi-threading for efficiency\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n    \n    # Create DataLoader for validation dataset without shuffling\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n    \n    return train_loader, val_loader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T09:35:35.834204Z","iopub.execute_input":"2025-01-21T09:35:35.834458Z","iopub.status.idle":"2025-01-21T09:35:35.844070Z","shell.execute_reply.started":"2025-01-21T09:35:35.834433Z","shell.execute_reply":"2025-01-21T09:35:35.843274Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Training Loop with Mixed Precision","metadata":{}},{"cell_type":"code","source":"# Initialize the gradient scaler for mixed precision training\nscaler = torch.cuda.amp.GradScaler()\n\ndef train_one_epoch(model, train_loader, criterion, optimizer, scheduler, device):\n    # Set the model to training mode (enables features like dropout)\n    model.train()\n    \n    # Variable to accumulate the loss over the epoch\n    running_loss = 0.0\n    \n    # Loop over batches of data in the training set\n    for images, labels in tqdm(train_loader):  # tqdm provides a progress bar\n        # Move images and labels to the specified device (GPU/CPU)\n        images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n        \n        # Zero the gradients of all optimized variables\n        optimizer.zero_grad()\n        \n        # Enable mixed precision with autocasting for faster computation\n        with torch.cuda.amp.autocast():\n            # Forward pass: Compute the model's outputs\n            outputs = model(images)\n            # Calculate the loss between the model's outputs and the true labels\n            loss = criterion(outputs, labels)\n        \n        # Scales the loss for mixed precision training, then backpropagate\n        scaler.scale(loss).backward()\n        \n        # Optimizer step: Adjust model parameters based on gradients\n        scaler.step(optimizer)\n        \n        # Updates the scale of the loss for the next iteration\n        scaler.update()\n        \n        # Update the learning rate scheduler based on the optimizer step\n        scheduler.step()\n        \n        # Accumulate the running loss, weighted by the batch size\n        running_loss += loss.item() * images.size(0)\n\n    # Return the average loss over the entire dataset (epoch)\n    return running_loss / len(train_loader.dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T09:35:35.845056Z","iopub.execute_input":"2025-01-21T09:35:35.845355Z","iopub.status.idle":"2025-01-21T09:35:35.898822Z","shell.execute_reply.started":"2025-01-21T09:35:35.845330Z","shell.execute_reply":"2025-01-21T09:35:35.897870Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/4138230182.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"# Disable gradient computation to save memory and computations during evaluation\n@torch.no_grad()\ndef validate(model, val_loader, criterion, device):\n    # Set the model to evaluation mode (disables dropout, batch norm, etc.)\n    model.eval()\n    \n    # Variable to accumulate the loss over the validation set\n    running_loss = 0.0\n    \n    # Lists to store the true labels and model outputs for performance evaluation\n    all_labels, all_outputs = [], []\n\n    # Loop over batches of validation data\n    for images, labels in tqdm(val_loader):  # tqdm provides a progress bar\n        # Move images and labels to the specified device (GPU/CPU)\n        images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n\n        # Forward pass: Compute the model's outputs (no gradient computation here)\n        outputs = model(images)\n        \n        # Calculate the loss between the model's outputs and the true labels\n        loss = criterion(outputs, labels)\n\n        # Accumulate the loss, weighted by the batch size\n        running_loss += loss.item() * images.size(0)\n        \n        # Append the true labels and model outputs to lists (to compute metrics later)\n        all_labels.append(labels.cpu().numpy())  # Move labels back to CPU for storage\n        all_outputs.append(outputs.cpu().numpy())  # Move outputs back to CPU for storage\n\n    # Concatenate the list of all labels and outputs into arrays for further evaluation\n    all_labels = np.concatenate(all_labels)\n    all_outputs = np.concatenate(all_outputs)\n\n    # Return the average loss over the entire validation dataset and the predictions/labels\n    return running_loss / len(val_loader.dataset), all_labels, all_outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T09:35:35.899976Z","iopub.execute_input":"2025-01-21T09:35:35.900324Z","iopub.status.idle":"2025-01-21T09:35:35.908665Z","shell.execute_reply.started":"2025-01-21T09:35:35.900282Z","shell.execute_reply":"2025-01-21T09:35:35.907859Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"def train_model(csv_file, img_dir, model, img_size=(224, 224), num_epochs=10, batch_size=32, lr=1e-4, n_fold=0, device='cuda'):\n    # Create the data loaders for training and validation from the provided CSV and image directory\n    train_loader, val_loader = create_dataloaders(csv_file, img_dir, img_size, batch_size, n_fold)\n\n    # Move the model to the specified device (e.g., GPU or CPU)\n    model = model.to(device)\n    \n    # Define the loss function (Binary Cross-Entropy with logits)\n    criterion = nn.BCEWithLogitsLoss()\n    \n    # Set up the optimizer (AdamW in this case)\n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    \n    # Set up the learning rate scheduler for dynamic learning rate adjustment during training\n    scheduler = OneCycleLR(optimizer, max_lr=lr, epochs=num_epochs, steps_per_epoch=len(train_loader))\n\n    # Variable to track the best validation ROC AUC score across all epochs\n    best_val_auc = 0\n    \n    # Loop over the specified number of epochs\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        \n        # Train the model for one epoch and get the training loss\n        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n        \n        # Validate the model after each epoch, get validation loss and predictions\n        val_loss, val_labels, val_outputs = validate(model, val_loader, criterion, device)\n\n        # Apply sigmoid to the model outputs to get probabilities (since BCEWithLogitsLoss is used)\n        val_outputs = torch.sigmoid(torch.tensor(val_outputs)).numpy()\n        \n        # Convert the probabilities into binary predictions (threshold at 0.5)\n        val_preds = (val_outputs > 0.5).astype(int)\n\n        # Compute performance metrics: accuracy, F1 score, and ROC AUC score\n        accuracy = accuracy_score(val_labels, val_preds)\n        f1 = f1_score(val_labels, val_preds)\n        roc_auc = roc_auc_score(val_labels, val_outputs)\n\n        # Print the metrics for this epoch\n        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}, F1: {f1:.4f}, ROC AUC: {roc_auc:.4f}\")\n\n        # If the current ROC AUC is the best we've seen, save the model\n        if roc_auc > best_val_auc:\n            print(\"Saving Best Model...\")\n            torch.save(model.state_dict(), f\"best_model_fold{n_fold}.pth\")\n            best_val_auc = roc_auc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T09:35:35.909636Z","iopub.execute_input":"2025-01-21T09:35:35.909926Z","iopub.status.idle":"2025-01-21T09:35:35.923654Z","shell.execute_reply.started":"2025-01-21T09:35:35.909901Z","shell.execute_reply":"2025-01-21T09:35:35.922828Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":" # Prediction","metadata":{}},{"cell_type":"code","source":"def predict(csv_file, img_dir, model, img_size=(224, 224), batch_size=32, device='cuda', model_path='best_model.pth'):\n    # Get the necessary transformations for validation (e.g., resizing, normalization, etc.)\n    _, val_transform = get_transforms(img_size)\n    \n    # Create a custom dataset using the CSV file and image directory with the validation transformations\n    dataset = CustomImageDataset(csv_file, img_dir, transform=val_transform)\n    \n    # Create a DataLoader for the validation dataset with batch processing\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n\n    # Load the model weights from the specified file (model_path)\n    model.load_state_dict(torch.load(model_path))\n    \n    # Move the model to the specified device (GPU or CPU)\n    model = model.to(device)\n    \n    # Set the model to evaluation mode (disables dropout, batch normalization, etc.)\n    model.eval()\n\n    # List to store the model's outputs (predictions)\n    outputs = []\n    \n    # Disable gradient calculation to save memory and computations during prediction\n    with torch.no_grad():\n        # Loop over the batches in the DataLoader\n        for images, _ in tqdm(loader):  # _ means labels are not needed for prediction\n            images = images.to(device)  # Move images to the appropriate device (GPU/CPU)\n            \n            # Forward pass: Get the model's predictions\n            preds = model(images)\n            \n            # Apply sigmoid to the outputs to get probabilities (for binary classification)\n            outputs.append(torch.sigmoid(preds).cpu().numpy())\n\n    # Concatenate all the predicted outputs from different batches into one array\n    return np.concatenate(outputs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T09:38:16.232647Z","iopub.execute_input":"2025-01-21T09:38:16.233452Z","iopub.status.idle":"2025-01-21T09:38:16.239189Z","shell.execute_reply.started":"2025-01-21T09:38:16.233421Z","shell.execute_reply":"2025-01-21T09:38:16.238329Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Set Parameters","metadata":{}},{"cell_type":"code","source":"\nimg_dir = \"/Train\"\ncsv_path = \"train.csv\"\nlabels = pd.read_csv(csv_path)\nlabels[\"label\"] = labels[\"label\"].map({\"editada\": 0, \"real\": 1})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T09:38:16.357535Z","iopub.execute_input":"2025-01-21T09:38:16.357792Z","iopub.status.idle":"2025-01-21T09:38:16.377401Z","shell.execute_reply.started":"2025-01-21T09:38:16.357769Z","shell.execute_reply":"2025-01-21T09:38:16.376657Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Initialize Model","metadata":{}},{"cell_type":"code","source":"\nmodel = timm.create_model('swin_base_patch4_window7_224', pretrained=True, num_classes=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T09:38:16.378692Z","iopub.execute_input":"2025-01-21T09:38:16.378956Z","iopub.status.idle":"2025-01-21T09:38:19.840316Z","shell.execute_reply.started":"2025-01-21T09:38:16.378931Z","shell.execute_reply":"2025-01-21T09:38:19.839389Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51166806096d440899d90fbce6deb173"}},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"\ntrain_model(labels, img_dir, model, img_size=(224, 224), num_epochs=30, batch_size=16, lr=1e-4, n_fold=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T09:38:19.841763Z","iopub.execute_input":"2025-01-21T09:38:19.842126Z","iopub.status.idle":"2025-01-21T09:45:40.234225Z","shell.execute_reply.started":"2025-01-21T09:38:19.842096Z","shell.execute_reply":"2025-01-21T09:45:40.233108Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:14<00:00,  2.49it/s]\n100%|██████████| 9/9 [00:01<00:00,  5.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7024, Val Loss: 0.6373, Accuracy: 0.6111, F1: 0.7407, ROC AUC: 0.7576\nSaving Best Model...\nEpoch 2/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.76it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5780, Val Loss: 0.5129, Accuracy: 0.6458, F1: 0.7606, ROC AUC: 0.9637\nSaving Best Model...\nEpoch 3/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.75it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3741, Val Loss: 0.0209, Accuracy: 1.0000, F1: 1.0000, ROC AUC: 1.0000\nSaving Best Model...\nEpoch 4/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.61it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2514, Val Loss: 0.1366, Accuracy: 0.9722, F1: 0.9747, ROC AUC: 0.9998\nEpoch 5/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.75it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2323, Val Loss: 0.0381, Accuracy: 0.9931, F1: 0.9938, ROC AUC: 0.9998\nEpoch 6/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.75it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2676, Val Loss: 0.0329, Accuracy: 0.9861, F1: 0.9878, ROC AUC: 1.0000\nEpoch 7/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.75it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1818, Val Loss: 0.0159, Accuracy: 1.0000, F1: 1.0000, ROC AUC: 1.0000\nEpoch 8/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.75it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1881, Val Loss: 0.0646, Accuracy: 0.9792, F1: 0.9818, ROC AUC: 1.0000\nEpoch 9/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.75it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2339, Val Loss: 0.1034, Accuracy: 0.9375, F1: 0.9474, ROC AUC: 0.9992\nEpoch 10/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.75it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2373, Val Loss: 0.1372, Accuracy: 0.9236, F1: 0.9272, ROC AUC: 1.0000\nEpoch 11/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.74it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1525, Val Loss: 0.0684, Accuracy: 0.9722, F1: 0.9747, ROC AUC: 1.0000\nEpoch 12/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.75it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1227, Val Loss: 0.0038, Accuracy: 1.0000, F1: 1.0000, ROC AUC: 1.0000\nEpoch 13/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.75it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1570, Val Loss: 0.0414, Accuracy: 0.9792, F1: 0.9814, ROC AUC: 0.9992\nEpoch 14/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.75it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0917, Val Loss: 0.0011, Accuracy: 1.0000, F1: 1.0000, ROC AUC: 1.0000\nEpoch 15/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.76it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1233, Val Loss: 0.0244, Accuracy: 0.9861, F1: 0.9877, ROC AUC: 0.9998\nEpoch 16/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.75it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0777, Val Loss: 0.0006, Accuracy: 1.0000, F1: 1.0000, ROC AUC: 1.0000\nEpoch 17/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.75it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1112, Val Loss: 0.0088, Accuracy: 1.0000, F1: 1.0000, ROC AUC: 1.0000\nEpoch 18/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.75it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0685, Val Loss: 0.0002, Accuracy: 1.0000, F1: 1.0000, ROC AUC: 1.0000\nEpoch 19/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.76it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1085, Val Loss: 0.0068, Accuracy: 1.0000, F1: 1.0000, ROC AUC: 1.0000\nEpoch 20/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.75it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0609, Val Loss: 0.0019, Accuracy: 1.0000, F1: 1.0000, ROC AUC: 1.0000\nEpoch 21/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.76it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0601, Val Loss: 0.0005, Accuracy: 1.0000, F1: 1.0000, ROC AUC: 1.0000\nEpoch 22/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.75it/s]\n100%|██████████| 9/9 [00:01<00:00,  5.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0372, Val Loss: 0.0163, Accuracy: 0.9931, F1: 0.9939, ROC AUC: 1.0000\nEpoch 23/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.75it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0249, Val Loss: 0.0054, Accuracy: 0.9931, F1: 0.9939, ROC AUC: 1.0000\nEpoch 24/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.76it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0192, Val Loss: 0.0001, Accuracy: 1.0000, F1: 1.0000, ROC AUC: 1.0000\nEpoch 25/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.74it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0184, Val Loss: 0.0005, Accuracy: 1.0000, F1: 1.0000, ROC AUC: 1.0000\nEpoch 26/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.76it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0301, Val Loss: 0.0030, Accuracy: 1.0000, F1: 1.0000, ROC AUC: 1.0000\nEpoch 27/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.76it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0200, Val Loss: 0.0010, Accuracy: 1.0000, F1: 1.0000, ROC AUC: 1.0000\nEpoch 28/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.75it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0196, Val Loss: 0.0007, Accuracy: 1.0000, F1: 1.0000, ROC AUC: 1.0000\nEpoch 29/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.76it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0095, Val Loss: 0.0005, Accuracy: 1.0000, F1: 1.0000, ROC AUC: 1.0000\nEpoch 30/30\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_30/4138230182.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n100%|██████████| 36/36 [00:13<00:00,  2.76it/s]\n100%|██████████| 9/9 [00:01<00:00,  6.37it/s]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0109, Val Loss: 0.0005, Accuracy: 1.0000, F1: 1.0000, ROC AUC: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"\ntest_dir = \"Test\"  #add your test path\ntest_csv = \"sample_submission.csv\"\ntest_labels = pd.read_csv(test_csv)\npreds = predict(test_labels, test_dir, model, img_size=(224, 224), batch_size=16, model_path='best_model_fold0.pth')\n\ntest_labels['label'] = preds\ntest_labels.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T09:45:40.236434Z","iopub.execute_input":"2025-01-21T09:45:40.237304Z","iopub.status.idle":"2025-01-21T09:45:42.401320Z","shell.execute_reply.started":"2025-01-21T09:45:40.237257Z","shell.execute_reply":"2025-01-21T09:45:42.400213Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/817557447.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n100%|██████████| 12/12 [00:01<00:00,  6.75it/s]\n","output_type":"stream"}],"execution_count":12}]}